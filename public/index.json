
[{"content":"","date":"1 April 2025","externalUrl":null,"permalink":"/blog/","section":"Blogs","summary":"","title":"Blogs","type":"blog"},{"content":"","date":"1 April 2025","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"1 April 2025","externalUrl":null,"permalink":"/","section":"Focus Breathing","summary":"","title":"Focus Breathing","type":"page"},{"content":"","date":"1 April 2025","externalUrl":null,"permalink":"/tags/homelab/","section":"Tags","summary":"","title":"Homelab","type":"tags"},{"content":"","date":"1 April 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Homelab is the name given to the server(s) that reside locally in your home that you can use for various functions such as virtualization, testing, NAS, media server etc. This server can be a raspberry pi, an old pc/laptop or any other computer.\nThe homelab architecture # The server # For my homelab server I use an Asrock Deskmeet x600, it is a mini ITX barebone unit. It is a great fit for a homelab due to its compact size. The specs are:\nCPU: Ryzen 7 5700G\nRAM: 64GB\nStorage:\nInternal - 500GB + 4TB External - 4TB + 1TB Network Attached Storage - 1TB Storage # Storage is an important part of any homelab. For me, I am using it as a media server, photos and drive. So I have a lot of storage for this data as well as for backups.\nInternal 1: This is a Crucial P3 500GB M.2 drive. It is used as the OS drive for the hypervisor and all the VMs. Internal 2: This is a Seagate Ironwolf 4TB hard drive and is used to store everything. External 1: This is a Seagate 4TB external hard drive. This is the backup drive. All important stuff is backed up to this drive daily. External 2: This is a 1TB drive from an old computer housed in a SATA to USB enclosure. It is used as my time machine backup disk. NAS: It\u0026rsquo;s a Synology NAS with a 1TB drive from another old computer. This stores the backup of all my VMs. Architecture # This is a high level architecture of my homelab.\nServices Running # All of the services that I am running are through Docker on Debian VMs. This enables easy onboarding and testing of these services and does not leave residue after uninstallation.\nUtilities # Nginx Proxy Manager # When starting out you can access your homelab resources with the ip and port combination but it gets very difficult to remember all the ips and ports to your service. Moreover, some services require you to run over SSL. Nginx Proxy Manager provides an easy to use GUI for nginx to be used as reverse proxy and handles SSL as well.\nAdguard # We use adblock on your browsers but that\u0026rsquo;s it, there is not option to block ads from other resources like on a smart TV or OS level telemetry. This is where a DNS based adblock like Adguard comes into play. It serves two purposes. Firstly, It blocks DNS queries to the Ad services essentially blocking the ad providing a network wide adblock. And second, it provides internal DNS records to my local resources. You just have to change the DHCP settings on your router to use adguard as the DNS server.\nVaultwarden # Creating a strong password for every site and then remembering it is not humanly possible. That is where password managers come in, but they are not free or very limited in their free tier. Vaultwarden is an alternative implementation of the Bitwarden Client API, written in Rust and compatible with the official bitwarden clients.\nIt-tools # Want to convert markdown to HTML, hash a piece of text, convert hex to rgb, prettify JSON etc? This is collection of all these tools and many more in a single application. No need to go to different sites or download multiple tools. I have a self hosted version of this here.\nSpeedtest tracker # It is simply a internet speedtest on a schedule. It can also send notifications when the speed is lower than a certain threshold.\nOpenMedia vault # Managing storage and backups in a homelab is a very crucial part. I have Open Media Vault (OMV) running in a VM that is used to centrally manage all the storage. OMV exposes network shares which can be mounted by other VMs and systems on my network. As all storage is now centralised I can easy backup this to the external hard drive using a software called borg backup. OMV also exposes the 1TB external hard drive as a samba share to be used time machine backup for my mac.\nCode Server # Write code from anywhere on VS Code from your browser. No need to install dependencies on your device. You can install this on a separate VM and get full access like you would in SSH.\nHome Assistant # Are you using smart devices in your house but are having to manage them though different apps. Home Assistant is an opensource home automation tool. It is very powerful can be integrated with a variety of a smart devices to create automations around it. I mainly use it to turn on/off lights on a schedule and monitor the cameras outside my house. But it can do far more than this.\nUptime Kuma # Homelab services do not provide high uptime as the cloud counterparts, its not a huge problem as it caters only to you and a few other and not thousands or millions of people. But what is important is that you should be able to know when a service is down. This is what Uptime Kuma does, it regularly pings your services and notifies you if any of them goes down.\nAs this is also hosted in my homelab and prone to downtime, so in addition I also use UptimeRobot. These are both very similar but uptimerobot is a service on cloud so it gives better uptime. You can see my current uptime here.\nPortainer # So now you have all these services running through docker but what if you need to restart a service or check for logs? You have to go to the homelab server and run docker commands to do so. Not anymore, use portainer for a GUI to quickly manage all the docker containers you are running.\nTwingate # Now we are running all these services on our homelab in our local network and all works fine when we are at home. But what happens if you are not. You can either port forward your services and make them public (security nightmare!! and not even possible for some ISPs) or you can use a VPN (low security risks but still not a feasible way for all ISPs). Twingate can be used no matter what is your use case. It is a VPN which can be setup with zero configuration.\nProductivity # Nextcloud # Nextcloud is a very well built google workspace alternative. It has all the features of google workspaces like drive, meet, calendar, mail inbox any much more. You can save a lot of money by self hosting nextcloud instead of paying for google drive.\nImmich # Most of us are either paying for icloud or google photos to backup photos. Not anymore, self host immich to fully replace any cloud photos backup. Unlike other alternative it has a very nice and easy to use UI too.\nGitea # We all have used github, but you need to remove all your secrets from the commits before pushing. A local github alternative might be useful incase you just need to keep all your files as is with secrets also. I use gitea as my local git server, all the git features work as expected and it has a nice UI to view your code.\nHomarr # Keeping track of all these services becomes a hassle when you have a quite a few like me. Homarr provides a pretty dashboard where you can add all your services and look at them at a quick glance. It also provices API integration with these services to show widgets.\nOllama with OpenWebUI # LLMs like ChatGPT are very popular these days. It can be useful in many tasks but what about privacy. And due to high demand chatgpt is down sometimes too. You can host your local LLM to use as your personal private chatbot.\nEntertainment # Jellyfin # Jellyfin is a media server. It organises all your downloaded media with the correct metadata and provides a web UI for playback.\n*arr Stack # Paying for multiple OTT subscriptions. Save money by hosting the *arr stack. I host sonarr, radarr and prowlarr in my homelab. You\u0026rsquo;ll also need to host a download client with these like qBittorrent.\nYou\u0026rsquo;ll still need to manually add the shows and movies which can be a hassle. To fully automate this you can use listrr.pro to create lists of movies and shows using filters. You can use my custom lists as a starting point: Movies and Shows.\n","date":"1 April 2025","externalUrl":null,"permalink":"/blog/what-is-a-homelab-and-why-you-should-consider-having-one/","section":"Blogs","summary":"","title":"What is a Homelab and why you should consider having one?","type":"blog"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/tags/astro/","section":"Tags","summary":"","title":"Astro","type":"tags"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/tags/decapcms/","section":"Tags","summary":"","title":"DecapCMS","type":"tags"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"Github","type":"tags"},{"content":"I created this tutorial as I could not find a simple and easy to integrate a CMS with a static site generator (SSG).\nAll the SSGs have a single issue that you have to edit the markdown directly. You need to have the project set up on your device. There is no admin interface to easily edit the content of the website. The tutorials available are not easy to understand and implement. But in this tutorial I\u0026rsquo;ll also show you how to add DecapCMS to an existing Astro project to edit the site using an admin interface.\nGetting started # For this tutorial we\u0026rsquo;ll need the following:\nNode.js (v22+). A Github and Vercel account (its free). A project in which you want to add DecapCMS. We\u0026rsquo;ll use the Astro blog template for this tutorial. A custom domain (not mandatory but good to have). Setting up the Astro site # Cloning the starter project # Create a new project with the Astro blog template. You can keep the directory name as blog.\nnpm create astro@latest -- --template blog After following the steps you\u0026rsquo;ll have the following directory structure.\nüì¶blog ‚î£ üìÇ.vscode ‚î£ üìÇpublic ‚î£ üìÇsrc ‚î£ üìú.gitignore ‚î£ üìúREADME.md ‚î£ üìúastro.config.mjs ‚î£ üìúpackage-lock.json ‚î£ üìúpackage.json ‚îó üìútsconfig.json Now, run the project.\nnpm dev And go to http://localhost:4321. You\u0026rsquo;ll be able to see the site.\nAdding a post # Now we\u0026rsquo;ll add a post to the site. Go to src/content/blog and create a file named test.md\n--- title: \u0026#34;Test 1\u0026#34; description: \u0026#34;created using code editor\u0026#34; pubDate: \u0026#34;Mar 27 2025\u0026#34; heroImage: \u0026#34;/blog-placeholder-4.jpg\u0026#34; --- This is a test post Save the file and go to the blog section of the site. Open the Test 1 post.\nDeploying the site # Your site is now ready to be deployed. First we\u0026rsquo;ll push the code to Github and then deploy it using vercel.\nPushing to Github # Create an empty repository in Github and push the code.\ngit remote add origin \u0026lt;your repository url\u0026gt; git branch -M main git push -u origin main Deploy on vercel # Login to Vercel using Github for easy configuration. Then add a new project. Choose Import Git Repository.\nFinally, click Deploy.\nWait for the site to get deployed. When the site is deployed click on the url to open the site.\nNow if you add a post and push to your repository, vercel will automatically build and deploy the site. But this becomes clunky if you do not have access to you machine with the repository setup locally. So to make it easier to add and edit posts well add DecapCMS so that you can edit the blog from anywhere through an interface in your browser only.\nAdding DecapCMS # Configuring DecapCMS # Create two files inside public/admin/\nüì¶public ‚î£ üìÇadmin ‚îÉ ‚î£ üìúconfig.yml ‚îÉ ‚îó üìúindex.html index.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;robots\u0026#34; content=\u0026#34;noindex\u0026#34; /\u0026gt; \u0026lt;link href=\u0026#34;/admin/config.yml\u0026#34; type=\u0026#34;text/yaml\u0026#34; rel=\u0026#34;cms-config-url\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Content Manager\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Include the script that builds the page and powers Decap CMS --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/decap-cms@^3.0.0/dist/decap-cms.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; config.yml\ncollections: - name: \u0026#34;blog\u0026#34; # Used in routes, e.g., /admin/collections/blog label: \u0026#34;Blog\u0026#34; # Used in the UI folder: \u0026#34;src/content/blog\u0026#34; # The path to the folder where the documents are stored create: true # Allow users to create new documents in this collection fields: # The fields for each document, usually in frontmatter - { label: \u0026#34;Title\u0026#34;, name: \u0026#34;title\u0026#34;, widget: \u0026#34;string\u0026#34; } - { label: \u0026#34;Description\u0026#34;, name: \u0026#34;description\u0026#34;, widget: \u0026#34;string\u0026#34; } - { label: \u0026#34;Publist date\u0026#34;, name: \u0026#34;pubDate\u0026#34;, widget: \u0026#34;datetime\u0026#34; } - { label: \u0026#34;Hero image\u0026#34;, name: \u0026#34;heroImage\u0026#34;, widget: \u0026#34;string\u0026#34; } media_folder: \u0026#34;src/assets/images\u0026#34; # Location where files will be stored in the repo public_folder: \u0026#34;src/assets/images\u0026#34; # The src attribute for uploaded media backend: name: github repo: \u0026lt;owner-name\u0026gt;/\u0026lt;repo-name\u0026gt; branch: main Now open http://localhost:4321/admin/index.html and click Login with GitHub.\nIf everything worked you will be able to see all your posts.\nEditing a post # Open the post we just created and edit it. When you are done click on Publish. DecapCMS will push a commit with the changes to your Github repository and vercel will start the build. Wait for the build to finish and see the changes.\nYou\u0026rsquo;ll not be able to edit the posts just yet from the public url on vercel. Commit and push all your changes to Github. We\u0026rsquo;ll need to configure the OAuth for production use.\nCreating OAuth for production # Create Github OAuth application # Login to Github and go to Profile \u0026gt; Developer Settings \u0026gt; OAuth Apps \u0026gt; New OAuth app.\nEnter the following values:\nApplication name: blog (can be anything) Homepage URL: vercel public url of the app Application Description: decap oauth (can be anything) Authorization callback URL: /api/callback Enable Device Flow: Keep unchecked Click Register. On the next page copy the Client ID and Client secret. We\u0026rsquo;ll need it later.\nAdding environment variables in vercel # In your vercel project go to Settings \u0026gt; Environment variables.\nEnter the following values:\nGITHUB_CLIENT_ID: copied from github. GITHUB_CLIENT_SECRET: copied from github. REDIRECT_URI: The url you used in github authorization callback url. Click Save.\nAdding API routes in application # We\u0026rsquo;ll use vercel functions to create an API to forward these requests to Github.\nCreate a directory named api in the root blog directory. Create two files auth.js and callback.js.\nauth.js\nexport function GET(request, res) { const CLIENT_ID = process.env.GITHUB_CLIENT_ID; const REDIRECT_URI = process.env.REDIRECT_URI; const authURL = `https://github.com/login/oauth/authorize?client_id=${CLIENT_ID}\u0026amp;redirect_uri=${REDIRECT_URI}\u0026amp;scope=repo,user`; return Response.redirect(authURL, 307); } callback.js\nimport axios from \u0026#34;axios\u0026#34;; export async function GET(req) { const CLIENT_ID = process.env.GITHUB_CLIENT_ID; const CLIENT_SECRET = process.env.GITHUB_CLIENT_SECRET; const REDIRECT_URI = process.env.REDIRECT_URI; const url = new URL(req.url); const params = new URLSearchParams(url.search); console.log(url, params); const code = params.get(\u0026#34;code\u0026#34;); console.log(code); if (!code) { return new Response( `\u0026lt;script\u0026gt;window.opener.postMessage({ error: \u0026#34;Missing code\u0026#34; }, \u0026#34;*\u0026#34;); window.close();\u0026lt;/script\u0026gt;`, ); } try { const tokenResponse = await axios.post( \u0026#34;https://github.com/login/oauth/access_token\u0026#34;, { client_id: CLIENT_ID, client_secret: CLIENT_SECRET, code, redirect_uri: REDIRECT_URI, }, { headers: { Accept: \u0026#34;application/json\u0026#34; } }, ); const accessToken = tokenResponse.data.access_token; const content = JSON.stringify({ token: accessToken, provider: \u0026#34;github\u0026#34; }); const message = JSON.stringify(`authorization:github:success:${content}`); return new Response( ` \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;script\u0026gt; (function() { function recieveMessage(e) { console.log(\u0026#34;recieveMessage %o\u0026#34;, e) // send message to main window with da app window.opener.postMessage( ${message}, e.origin ) } window.addEventListener(\u0026#34;message\u0026#34;, recieveMessage, false) // Start handshare with parent console.log(\u0026#34;Sending message: %o\u0026#34;, \u0026#34;github\u0026#34;) window.opener.postMessage(\u0026#34;authorizing:github\u0026#34;, \u0026#34;*\u0026#34;) })() \u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; `, { headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;text/html\u0026#34; } }, ); } catch (error) { const content = JSON.stringify(error); const message = JSON.stringify(`authorization:github:error:${content}`); return new Response(` \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;script\u0026gt; (function() { function recieveMessage(e) { console.log(\u0026#34;recieveMessage %o\u0026#34;, e) // send message to main window with da app window.opener.postMessage( ${message}, e.origin ) } window.addEventListener(\u0026#34;message\u0026#34;, recieveMessage, false) // Start handshare with parent console.log(\u0026#34;Sending message: %o\u0026#34;, \u0026#34;github\u0026#34;) window.opener.postMessage(\u0026#34;authorizing:github\u0026#34;, \u0026#34;*\u0026#34;) })() \u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; `); } } The directory structure will look like this:\nüì¶blog ‚î£ üìÇapi ‚îÉ ‚î£ üìúauth.js ‚îÉ ‚îó üìúcallback.js Add production and development config # Make a copy of the public/admin/config.yml as config.dev.yml and config.prod.yml.\nconfig.dev.yml\ncollections: - name: \u0026#34;blog\u0026#34; # Used in routes, e.g., /admin/collections/blog label: \u0026#34;Blog\u0026#34; # Used in the UI folder: \u0026#34;src/content/blog\u0026#34; # The path to the folder where the documents are stored create: true # Allow users to create new documents in this collection fields: # The fields for each document, usually in frontmatter - { label: \u0026#34;Title\u0026#34;, name: \u0026#34;title\u0026#34;, widget: \u0026#34;string\u0026#34; } - { label: \u0026#34;Description\u0026#34;, name: \u0026#34;description\u0026#34;, widget: \u0026#34;string\u0026#34; } - { label: \u0026#34;Publist date\u0026#34;, name: \u0026#34;pubDate\u0026#34;, widget: \u0026#34;datetime\u0026#34; } - { label: \u0026#34;Hero image\u0026#34;, name: \u0026#34;heroImage\u0026#34;, widget: \u0026#34;string\u0026#34; } media_folder: \u0026#34;src/assets/images\u0026#34; # Location where files will be stored in the repo public_folder: \u0026#34;src/assets/images\u0026#34; # The src attribute for uploaded media backend: name: github repo: \u0026lt;owner-name\u0026gt;/\u0026lt;repo-name\u0026gt; branch: main config.prod.yml\ncollections: - name: \u0026#34;blog\u0026#34; # Used in routes, e.g., /admin/collections/blog label: \u0026#34;Blog\u0026#34; # Used in the UI folder: \u0026#34;src/content/blog\u0026#34; # The path to the folder where the documents are stored create: true # Allow users to create new documents in this collection fields: # The fields for each document, usually in frontmatter - { label: \u0026#34;Title\u0026#34;, name: \u0026#34;title\u0026#34;, widget: \u0026#34;string\u0026#34; } - { label: \u0026#34;Description\u0026#34;, name: \u0026#34;description\u0026#34;, widget: \u0026#34;string\u0026#34; } - { label: \u0026#34;Publist date\u0026#34;, name: \u0026#34;pubDate\u0026#34;, widget: \u0026#34;datetime\u0026#34; } - { label: \u0026#34;Hero image\u0026#34;, name: \u0026#34;heroImage\u0026#34;, widget: \u0026#34;string\u0026#34; } media_folder: \u0026#34;src/assets/images\u0026#34; # Location where files will be stored in the repo public_folder: \u0026#34;src/assets/images\u0026#34; # The src attribute for uploaded media backend: name: github repo: \u0026lt;owner-name\u0026gt;/\u0026lt;repo-name\u0026gt; branch: main base_url: \u0026lt;astro public url\u0026gt; auth_endpoint: api/auth Edit the scripts in package.json as follows:\n\u0026#34;scripts\u0026#34;: { ... \u0026#34;build\u0026#34;: \u0026#34;cp public/admin/config.prod.yml public/admin/config.yml \u0026amp;\u0026amp; astro build\u0026#34;, \u0026#34;develop\u0026#34;: \u0026#34;cp public/admin/config.dev.yml public/admin/config.yml \u0026amp;\u0026amp; vercel dev\u0026#34;, ... }, Now install the following packages\nnpm i -g vercel npm i next@latest axios Check that everything is working locally. You\u0026rsquo;ll asked to login to vercel. Follow the steps to link the existing project on vercel to your local project.\nnpm run develop If everything worked you\u0026rsquo;ll still be able to open http://localhost:4321/admin/index.html.\nFinally deploying everything # Now we\u0026rsquo;ll finally commit and push everything again. Wait for the deployment to finish on vercel. Then navigate to /admin/index.html. If all went good you\u0026rsquo;ll be able to login using Github and see your posts.\nTry to edit a post and publish. The changes will reflected on the site after the build is completed in vercel.\nThis blog is a bit long. Thank you for making it till the end.üôè\nShare with others who might find this useful too.\n","date":"27 March 2025","externalUrl":null,"permalink":"/blog/modify-static-site-content-easily-from-your-browser/","section":"Blogs","summary":"","title":"Modify static site content easily from your browser","type":"blog"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/tags/oauth/","section":"Tags","summary":"","title":"OAuth","type":"tags"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/tags/serverless/","section":"Tags","summary":"","title":"Serverless","type":"tags"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/tags/ssg/","section":"Tags","summary":"","title":"SSG","type":"tags"},{"content":"","date":"27 March 2025","externalUrl":null,"permalink":"/tags/vercel/","section":"Tags","summary":"","title":"Vercel","type":"tags"},{"content":"","date":"25 March 2025","externalUrl":null,"permalink":"/tags/chatgpt/","section":"Tags","summary":"","title":"Chatgpt","type":"tags"},{"content":"","date":"25 March 2025","externalUrl":null,"permalink":"/tags/deepseek-r1/","section":"Tags","summary":"","title":"Deepseek R1","type":"tags"},{"content":"Hosting your own local LLM can be fast, secure and private to online alternative.\nBefore getting started # You\u0026rsquo;ll need a fairly decent machine if you want quicker responses. But as long as you are using it alone you can get by with a old machine also. We\u0026rsquo;ll use docker to host the LLM and web ui so make sure docker is installed and running. Installing Ollama # In this tutorial we\u0026rsquo;ll use Ollama to host the LLM and later connect an UI. Create a directory called chat and create a compose.yml file inside it.\nmkdir chat cd chat touch compose.yml Add the following to the compose.yml file.\nservices: ollama: image: ollama/ollama container_name: ollama restart: unless-stopped ports: - 11434:11434 volumes: - ./ollama-data:/root/.ollama Now, run the container.\ndocker compose up -d And go to :11434. You\u0026rsquo;ll be able to see ollama is running.\nAdding the UI for chat # For the UI we\u0026rsquo;ll use the open web UI project. To add this we\u0026rsquo;ll edit the compose.yml file.\nservices: ollama: image: ollama/ollama container_name: ollama restart: unless-stopped ports: - 11434:11434 volumes: - ./ollama-data:/root/.ollama open-webui: image: ghcr.io/open-webui/open-webui:main restart: unless-stopped container_name: ollama-webui volumes: - ./open-webui-data:/app/backend/data environment: - \u0026#34;OLLAMA_BASE_URL=http://ollama:11434\u0026#34; ports: - 3000:8080 Then re-run the containers.\ndocker compose down docker compose up -d Now, navigate to :3000 and create a admin account.\nDownloading a LLM # Now we have all the essentials setup and running to download a use a LLM model. You can any model from the Ollama website. For this tutorial we\u0026rsquo;ll use the deelseek-r1 model with 1.5 billion parameters.\nOn the web interface go to select model and search deepseek-r1:1.5b and click pull to begin downloading. Wait for the model to get downloaded.\nOnce the model is downloaded select the model. Try to ask it a joke üòÇ\nOkay! the jokes might not be as funny üòÖ but you have your own private chatbot.\n","date":"25 March 2025","externalUrl":null,"permalink":"/blog/how-to-host-a-local-llm-model/","section":"Blogs","summary":"","title":"How to host a local LLM model","type":"blog"},{"content":"","date":"25 March 2025","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"25 March 2025","externalUrl":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama","type":"tags"},{"content":"","date":"25 March 2025","externalUrl":null,"permalink":"/tags/open-webui/","section":"Tags","summary":"","title":"Open Webui","type":"tags"},{"content":"","date":"23 March 2025","externalUrl":null,"permalink":"/tags/email/","section":"Tags","summary":"","title":"Email","type":"tags"},{"content":"","date":"23 March 2025","externalUrl":null,"permalink":"/tags/gmail/","section":"Tags","summary":"","title":"Gmail","type":"tags"},{"content":"In this tutorial we\u0026rsquo;ll use the terminal to send email using SMTP. This method can be used to test the SMTP credentials by sending a test email.\nBefore you start # You\u0026rsquo;ll need a UNIX based terminal with openssl installed. SMTP credentials from any service like Gmail, AWS SES, Mailchimp etc. We\u0026rsquo;ll use Gmail credentials in this example Encode the SMTP credentials # Base64 encode the SMTP username and password (password will be app password for gmail). Copy the result of each of the commands.\necho -n \u0026#34;\u0026lt;username\u0026gt;\u0026#34; | openssl enc -base64 echo -n \u0026#34;\u0026lt;password\u0026gt;\u0026#34; | openssl enc -base64 Creating the email body # Create a file named mail.txt with the following content.\nEHLO example.com AUTH LOGIN \u0026lt;base64 username\u0026gt; \u0026lt;base64 password\u0026gt; MAIL FROM: your@gmail.com RCPT TO: receiver@example.com DATA From: Sender Name \u0026lt;your@gmail.com\u0026gt; To: receiver@example.com Subject: Did it work? This message was sent using the terminal. . QUIT Replace your@gmail.com with your gmail id and receiver@example.com with the email id you want to send the message.\nSending the email # Run the following command to send the email.\nopenssl s_client -crlf -quiet -starttls smtp -connect \u0026lt;email server\u0026gt;:\u0026lt;port\u0026gt; \u0026lt; /path/to/mail.txt Replace your mail server host and port provided to you by the service.\nCheck your receiver mailbox.\nCongratulations!üéâ You\u0026rsquo;ve successfully sent email from the terminal.\n","date":"23 March 2025","externalUrl":null,"permalink":"/blog/how-to-send-email-using-cli/","section":"Blogs","summary":"","title":"How to send email using terminal","type":"blog"},{"content":"","date":"23 March 2025","externalUrl":null,"permalink":"/tags/smtp/","section":"Tags","summary":"","title":"SMTP","type":"tags"},{"content":"","date":"23 March 2025","externalUrl":null,"permalink":"/tags/terminal/","section":"Tags","summary":"","title":"Terminal","type":"tags"},{"content":"So you\u0026rsquo;ve built a site using Astro but can\u0026rsquo;t figure out how to add search functionality to the site without services like Algolia. Fear not in this tutorial we\u0026rsquo;ll add a full text search to any Astro website. It is simple and fast.\nWe\u0026rsquo;ll use the Pagefind library to implement the search feature in our astro website. You\u0026rsquo;ll need a working astro website. For this tutorial we\u0026rsquo;ll install and use the astro blog starter template.\nInstalling packages # We\u0026rsquo;ll install only one library for this tutorial.\nnpm i astro-pagefind Update configuration # Import the package in the astro.config.mjs\n... import pagefind from \u0026#34;astro-pagefind\u0026#34;; ... Also add pagefind in the integrations.\n... export default defineConfig({ site: \u0026#34;https://example.com\u0026#34;, integrations: [ ... pagefind(), ... ], }); ... Add the UI element # We\u0026rsquo;ll add the search bar to the blog page.\nEdit src/pages/blog/index.astro and add the following code.\n--- ... import Search from \u0026#34;astro-pagefind/components/Search\u0026#34;; ... --- ... \u0026lt;div style=\u0026#34;margin-bottom: 2rem;\u0026#34;\u0026gt; \u0026lt;Search id=\u0026#34;search\u0026#34; className=\u0026#34;pagefind-ui\u0026#34; uiOptions={{ showImages: false }} /\u0026gt; \u0026lt;/div\u0026gt; ... Now, you\u0026rsquo;ll be able to see the search bar in the blog page.\nGenerating the search index # If you try to search you won\u0026rsquo;t be abe to see any results now because the search does not work in the development mode. To make it the search work development mode we need to build the site once. Then we can use the search in the development mode also.\nnpm build npm run dev Now, search for ay blog. You\u0026rsquo;ll be able to full text search on any page.\nWoohoo!!ü•≥ Search is now working on your astro site.\n","date":"22 March 2025","externalUrl":null,"permalink":"/blog/add-search-to-any-astro-site/","section":"Blogs","summary":"","title":"Add search to any Astro site","type":"blog"},{"content":"","date":"22 March 2025","externalUrl":null,"permalink":"/tags/pagefind/","section":"Tags","summary":"","title":"Pagefind","type":"tags"},{"content":"","date":"22 March 2025","externalUrl":null,"permalink":"/tags/static-site-search/","section":"Tags","summary":"","title":"Static Site Search","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/cloudflare/","section":"Tags","summary":"","title":"Cloudflare","type":"tags"},{"content":"Wanting to host a website of your own but do not want to pay for servers or do not have a static public IP address. Yes you can probably use any other free services to achieve this and will probably get great uptime but where\u0026rsquo;s the fun in that right? You just need a domain to get started and expose any application to the internet for free!\nLet\u0026rsquo;s get started!\nBefore you begin # A machine with docker installed (preferably a UNIX based system). A domain that you own (you can get a .tk domain name for free). Basic knowledge about DNS settings. Setting up Docker # Install docker on a machine that you want to host your site on. Make sure this machine is connected to the internet (preferably using a ethernet cable) and any sleep settings are turned off if you are using a laptop. Just make sure that this machine is always on connected to the internet. For this tutorial we\u0026rsquo;ll be host a simple webpage but this can be extended to any web application.\nCreate a folder named my-site on your machine that you installed docker.\nmkdir my-site cd my-site Now we create a landing page for our site inside my-site/public/index.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; We are live!! \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Next we need to create a file named compose.yml this will contain the docker configuration. For this example we are using only two containers nginx and cloudflare. But you can use as many as required by your application like MySQL, redis etc.\nservices: my-site: image: nginx:mainline-alpine-slim container_name: my-site restart: on-failure ports: - 8080:80 volumes: - ./public:/usr/share/nginx/html/:ro The folder structure should look like this:\nüì¶my-site ‚î£ üìÇpublic ‚îÉ ‚îó üìúindex.html ‚îó üìúcompose.yml Now run the container to see if everything works on you local machine.\ndocker compose up -d Navigate to http://localhost:8080 on your machine and you\u0026rsquo;ll be able to see the site.\nSetting up Cloudflare # You should use Cloudflare\u0026rsquo;s nameservers for this to work. If you are not switch your nameservers to cloudflare.\nNow on cloudflare\u0026rsquo;s dashboard go to Zero Trust \u0026gt; Networks \u0026gt; Tunnels \u0026gt; Create a tunnel. Select cloudflared as the tunnel type.\nNext, give your tunnel a name.\nIn the configure section, copy the token from the docker command.\nSetting up the tunnel # Now edit the compose.yml file to include the cloudflare tunnel.\nservices: cf-tunnel: image: cloudflare/cloudflared:latest container_name: cf-tunnel restart: on-failure command: \u0026#34;tunnel --no-autoupdate run --token ${CF_TOKEN}\u0026#34; my-site: depends_on: - cf-tunnel image: nginx:mainline-alpine-slim container_name: my-site restart: on-failure ports: - 8080:80 volumes: - ./public:/usr/share/nginx/html/:ro Also create a .env file to store the token\nCF_TOKEN=\u0026lt;your-cloudflare-token\u0026gt; Now, run the new docker configuration.\ndocker compose up -d If everything worked you should see the connector in the cloudflare tunnel page.\nNext we have to configure the domain (or subdomain) that we want to map to this tunnel. For this tutorial I have kept the subdomain as my-site but you can leave it blank if you want to host this site at the root domain. In the service use HTTP for protocol and my-site:80 as the URL. Replace my-site with the container name in your compose.yml file.\nClick Save Tunnel and open the domain you just configured in the tunnel.\nHurray!üéâ You are live from your home.\n","date":"20 March 2025","externalUrl":null,"permalink":"/blog/host-websites-from-your-home/","section":"Blogs","summary":"","title":"Host websites from your home","type":"blog"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/networking/","section":"Tags","summary":"","title":"Networking","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/nginx/","section":"Tags","summary":"","title":"Nginx","type":"tags"},{"content":"","date":"20 March 2025","externalUrl":null,"permalink":"/tags/web-hosting/","section":"Tags","summary":"","title":"Web Hosting","type":"tags"},{"content":" Nipun Haldar # My role involves architecting scalable and reliable cloud infrastructures tailored to meet the specific needs and goals of my clients. Additionally, I develop custom solutions and automation scripts using Python to streamline processes, enhance efficiency, and drive innovation. From building web applications to creating data processing pipelines, I harness the power of Python to solve complex problems and deliver impactful solutions. My versatile skill set allows me to bridge the gap between cloud architecture and application development, enabling seamless integration across the entire technology stack.\nSkills # Development: AWS, Python, Linux, Docker, Shell, Javascript, Git Design Tools: Figma Soft Skills: Team player Strong communication Problem-solving Attention to detail Time management Work experience # Mar 2022 - Present Cloud Lead \u0026 Senior Software Engineer Samarth eGov, ICT Initiative of Ministry of Education Currently also heading the Cloud infrastructure management and DevOps. Designed a highly efficient seat allocation algorithm in Python for undergraduate and postgraduate programmes across all major universities in India. Successfully planned, developed and deployed cloud infrastructure for CUET in AWS, the second largest exam in India. Ensuring uptime of more than 500 in-house SaaS applications. Built a blockchain application to disburse 1.2 Lakh digital degrees on blockchain for University of Delhi. Created a face detection service to minimize unidentifiable faces in candidate application, saving time during verification. Aug 2020 - Feb 2021 Software Engineer\nSamarth eGov, ICT Initiative of Ministry of Education Developed an email service which sends millions of emails per day using AWS Lambda. Developed an application to manage and update more than 100 EC2 instances using Ansible. Self-hosted GitLab on AWS for our team, making project management more efficient and standardised. Developed numerous other services to take the load off of our main application stack and provide better performance. Streamlined DevOps processes and developed custom tools to improve internal workflow. Jun 2019 - Jul 2019 Software Development Intern\nInsitute of Informatics and Communication, University of Delhi Made a chess server with cross-platform user interfaces. Frontend - React.js and React Native Backend - Express.js and socket.io Databases - Redis and MongoDB Education # 2016-2020 Bachelor's degree in Electronics and Communication Technology Indian Institute of Information Technology, Trichy ","externalUrl":null,"permalink":"/about/","section":"","summary":"","title":"","type":"about"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]